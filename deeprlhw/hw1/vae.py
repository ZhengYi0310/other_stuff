import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import os
import ipdb as pdb

tf.set_random_seed(0)

z_dim=3 # latent space dimensionality
eps=1e-9 # numerical stability

def orthogonal_initializer(scale = 1.1):
    """
    reference from Lasagne and Keras, Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, 2013
    """
    def _initializer(shape, dtype=tf.float32):
        flat_shape = (shape[0], np.prod(shape[1:]))
        a = np.random.normal(0.0, 1.0, flat_shape)
        u, _, v = np.linalg.svd(a, full_matrices=False)
        # pick the one with the correct shape
        q = u if u.shape == flat_shape else v
        q = q.reshape(shape)
        print('Warning -- You have opted to use the orthogonal_initializer function')
        return tf.constant(scale * q[:shape[0], :shape[1]], dtype=tf.float32)

class NormalDistribution(object):
    """
    Represent a multivariate Gaussian distribution parameterized by (mu, Cov).
    . If Cov matrix is diagonal, Cov = (sigma).^2. Otherwisr, Cov = A*(sigma).^2*A, where 
    A = (I+v*r^T).
    """
    def __init__(self, mu, sigma, logsigma, v=None, r=None):
        self.mu = mu
        self.sigma = sigma
        self.logsigma = logsigma
        dim = mu.get_shape()
        if v is None:
            v = tf.constant(0., shape=dim)
        if r is None:
            r = tf.constant(0., shape=dim)
        self.v = v
        self.r = r

def linear(x, output_dim):
    W = tf.get_variable("W", [x.get_shape()[1], output_dim], initializer=orthogonal_initializer(1.1))
    b = tf.get_variable("b", [output_dim], initializer=tf.constant_initializer(0.0))
    return tf.matmul(x, W) +b

def Relu(x, output_dim, scope):
    # helper function for implementing stacked ReLU layers
    with tf.variable_scope(scope):
        return tf.nn.relu(linear(x, output_dim))

def encode(x, share=None):
    with tf.variable_scope("encoder",reuse=share):
        for l in range(2):
            if (l==0):
                x = Relu(x, 500, "l" + str(l))
            if (l==1):
                x = Relu(x, 128, "l" + str(l))
        return linear(x, 2 * z_dim)

def KLGaussian(Q, N):
    """
    Implementation of KL Divergence term KL(N0, N1) derived in Appendix A.1 from the embed to control paper.
    :param Q: Normal(mu, A*sigma_0*A^T), 
    :param N: Normal(mu, A*sigma_1*A^T), 
    :return: scalar divergence, measured in nats (information units under log rather than log2), shape= batch x 1
    """
    sum = lambda x: tf.reduce_sum(x, 1) # convenience function for summing over features (cols)
    k = float(Q.mu.get_shape()[1].value) # dimension of the distribution
    mu0, v, r, mu1 = Q.mu, Q.v, Q.r, N.mu
    sigma_0, sigma_1 = tf.square(Q.sigma), tf.square(N.sigma) + eps
    a = sum(sigma_0 * (1. + 2. * v * r) /sigma_1) + sum(tf.square(r) * sigma_0) * sum(tf.square(v) / sigma_1) # trace term
    b = sum(tf.square(mu1 - mu0) / sigma_1) # difference-of-means term
    c = 2. * (sum(N.logsimgma - Q.logsigma) - tf.log(1. + sum(v * r))) # ratio-of-determinants term.
    return 0.5 * (a + b - k + c)  # , a, b, c

def sampleNormal(mu, sigma):
    # diagonal stdev
    n01 = tf.random_normal(sigma.get_shape(), mean=0, stddev=1)
    return mu + sigma * n01

def sampleQ_phi(h_enc, share=None):
    with tf.variable_scope("sampleQ_phi", reuse=share):
        mu, log_sigma = tf.split(1, 2, linear(h_enc, z_dim * 2))  # diagonal stdev values
        sigma = tf.exp(log_sigma)
        return sampleNormal(mu, sigma), NormalDistribution(mu, sigma, log_sigma)

def transition(z, u):
    with tf.variable_scope("trans"):
        with tf.variable_scope("linear_state"):
            s1 = linear(z, z_dim)
        with tf.variable_scope("linear_action"):
            s2 = linear(u, z_dim)
        return s1 + s2


def sampleQ_psi(z,u):
  A,B,o,v,r=transition(z)
  with tf.variable_scope("sampleQ_psi"):
    mu_t=tf.expand_dims(Q_phi.mu,-1) # batch,z_dim,1
    Amu=tf.squeeze(tf.batch_matmul(A,mu_t), [-1])
    u=tf.expand_dims(u,-1) # batch,u_dim,1
    Bu=tf.squeeze(tf.batch_matmul(B,u),[-1])
    Q_psi=NormalDistribution(Amu+Bu+o,Q_phi.sigma,Q_phi.logsigma, v, r)
    # the actual z_next sample is generated by deterministically transforming z_t
    z=tf.expand_dims(z,-1)
    Az=tf.squeeze(tf.batch_matmul(A,z),[-1])
    z_next=Az+Bu+o
    return z_next,Q_psi#,(A,B,o,v,r) # debugging